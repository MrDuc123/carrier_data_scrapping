{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2cTircjM-O7",
        "outputId": "1c6c108f-f40e-487e-f509-2c9d81fae9e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching data: 100%|██████████| 69/69 [00:53<00:00,  1.30it/s]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "\n",
        "def slugify(text):\n",
        "    \"\"\"\n",
        "    Convert a string to a slug by:\n",
        "    - Removing diacritics (e.g., converting 'Đà Nẵng' to 'Da Nang')\n",
        "    - Lowercasing\n",
        "    - Replacing non-alphanumeric characters with hyphens\n",
        "    \"\"\"\n",
        "    # Normalize to remove accents\n",
        "    # Replace special characters Đ and đ with D and d\n",
        "    text = text.replace('Đ', 'D').replace('đ', 'd')\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
        "    text = text.lower()\n",
        "    # Replace any non-alphanumeric characters with hyphens and remove extra hyphens\n",
        "    text = re.sub(r'[^a-z0-9]+', '-', text).strip('-')\n",
        "    return text\n",
        "\n",
        "\n",
        "# Function to fetch data from the API\n",
        "def fetch_data(token, type_list, range_start, range_end):\n",
        "    results = []  # List to store each record as a dictionary\n",
        "    url = \"https://internal-vroute-cmc.vexere.com/v1/goyolo/area/\"\n",
        "    # Provided Bearer token\n",
        "\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\"\n",
        "    }\n",
        "    for i in tqdm(range(range_start,range_end), desc=\"Fetching data\"):\n",
        "        name = None\n",
        "        slug = None\n",
        "        url = f\"https://internal-vroute-cmc.vexere.com/v1/goyolo/area/{str(i)}\"\n",
        "        response = requests.get(url, headers=headers)\n",
        "        # Check if the HTTP request was successful\n",
        "        if response.status_code == 401:\n",
        "            print(\"Token expired\")\n",
        "            return None\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            if data['data']['type'] not in type_list:\n",
        "                continue\n",
        "            # Check if the returned message is \"success\"\n",
        "            if data.get(\"message\") == \"success\":\n",
        "              try:\n",
        "                # Extract and print the name from the \"data\" field\n",
        "                name = data.get(\"data\", {}).get(\"name\")\n",
        "                # Create a slug from the name, if available.\n",
        "                slug = slugify(name)\n",
        "                results.append({\"id\": i, \"name\": name, \"slug\": slug})\n",
        "              except Exception as e:\n",
        "                continue\n",
        "    # Convert the list of dictionaries to a DataFrame\n",
        "    a = pd.DataFrame(results)\n",
        "    a.to_csv(\"mapping.csv\", index=False)\n",
        "\n",
        "    return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    token = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ0eXAiOjIsInVzciI6ImZlIiwiY2lkIjoiYTRlYWM1MDAtMzYyNC0xMWU1LWFjOWUtMDkxMjRjNjAxMDEzIiwiZXhwIjoxNzQxNDM0Njc1fQ.IOUTipBcmHnjnQ9Mge0guQuJBuk9T8Uz6kKnFRPq8_0\"\n",
        "    type_list = [3]\n",
        "    fetch_data(token, type_list, 1, 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Pd-qkvtgM-O9"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "def slugify(text):\n",
        "    \"\"\"\n",
        "    Convert a string to a slug:\n",
        "    - Remove diacritics (e.g., 'Đà Nẵng' → 'Da Nang')\n",
        "    - Convert to lowercase\n",
        "    - Replace non-alphanumeric characters with hyphens\n",
        "    \"\"\"\n",
        "    text = text.replace('Đ', 'D').replace('đ', 'd')\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
        "    text = text.lower()\n",
        "    return re.sub(r'[^a-z0-9]+', '-', text).strip('-')\n",
        "\n",
        "\n",
        "def format_api_date(date_str):\n",
        "    \"\"\"\n",
        "    Convert a date from 'DD-MM-YYYY' format to the API format: 'YYYY-MM-DDT00:00:00+07:00'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        dt = datetime.strptime(date_str, \"%d-%m-%Y\")\n",
        "        return dt.strftime(\"%Y-%m-%dT00:00:00+07:00\")\n",
        "    except ValueError:\n",
        "        raise ValueError(\"Invalid date format. Use 'DD-MM-YYYY'.\")\n",
        "\n",
        "\n",
        "def get_bus_trip_count(url):\n",
        "\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(\"Failed to retrieve the page.\")\n",
        "        return None\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Find the element with class \"text-result-number\"\n",
        "    result_element = soup.find(class_=\"text-result-number\")\n",
        "\n",
        "    if result_element:\n",
        "        # Get the text and strip any surrounding whitespace\n",
        "        count_text = result_element.get_text(strip=True)\n",
        "        count_text = count_text.split()\n",
        "        return count_text[0]\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def get_total(from_destination, to_destination, date=\"27-02-2025\", mapping_csv=\"mapping.csv\"):\n",
        "    \"\"\"\n",
        "    Given departure and destination names, enrich the data by:\n",
        "      - Reading the mapping CSV (which has columns: id, name, slug),\n",
        "      - Normalizing the destination names using slugify,\n",
        "      - Building the route code and URL,\n",
        "      - Fetching the total bus trip count from the URL.\n",
        "\n",
        "    Returns a DataFrame with columns: from_name, to_name, total_trip, date.\n",
        "    \"\"\"\n",
        "    # Read the mapping CSV into a DataFrame.\n",
        "    city_df = pd.read_csv(mapping_csv)\n",
        "\n",
        "    # Normalize the destination names using slugify.\n",
        "    normalized_from = slugify(from_destination)\n",
        "    normalized_to = slugify(to_destination)\n",
        "\n",
        "    # Look up the corresponding rows in the mapping CSV.\n",
        "    row_from = city_df[city_df['slug'] == normalized_from]\n",
        "    row_to = city_df[city_df['slug'] == normalized_to]\n",
        "\n",
        "    if row_from.empty or row_to.empty:\n",
        "        print(\"Error: One or both destinations not found in mapping CSV.\")\n",
        "        return None\n",
        "\n",
        "    # Extract required fields.\n",
        "    from_id = row_from.iloc[0]['id']\n",
        "    from_slug = row_from.iloc[0]['slug']\n",
        "    to_id = row_to.iloc[0]['id']\n",
        "    to_slug = row_to.iloc[0]['slug']\n",
        "\n",
        "    # Generate the route code.\n",
        "    route_code = f\"1{from_id}t1{to_id}1\"\n",
        "\n",
        "    # Build the URL.\n",
        "    url = (\n",
        "        f\"https://vexere.com/vi-VN/ve-xe-khach-tu-{from_slug}-di-{to_slug}-\"\n",
        "        f\"{route_code}.html?date={date}\"\n",
        "    )\n",
        "\n",
        "    # Fetch the total trip count using the provided function.\n",
        "    total_trip = get_bus_trip_count(url)\n",
        "\n",
        "    return total_trip\n",
        "\n",
        "\n",
        "\n",
        "def get_city_id(city_df, city_name):\n",
        "    \"\"\"\n",
        "    Retrieve the city ID from the mapping CSV based on the city name.\n",
        "    \"\"\"\n",
        "    city_row = city_df[city_df['slug'] == slugify(city_name)]\n",
        "    if not city_row.empty:\n",
        "        return city_row.iloc[0, 0]  # Assuming city ID is in the first column\n",
        "    raise ValueError(f\"City '{city_name}' not found in the mapping file.\")\n",
        "\n",
        "\n",
        "\n",
        "def fetch_bus_data(token, from_id, to_id, date):\n",
        "    \"\"\"\n",
        "    Fetch bus data from the API for a given route and date.\n",
        "    Implements pagination to retrieve more than 100 records.\n",
        "    \"\"\"\n",
        "\n",
        "    # print(token)\n",
        "    api_url = \"https://internal-vroute-cmc.vexere.com/v2/route\"\n",
        "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "    all_data = []\n",
        "    page = 1\n",
        "    pagesize = 100  # adjust if needed\n",
        "    while True:\n",
        "        query_params = {\n",
        "            \"filter[from]\": from_id,\n",
        "            \"filter[to]\": to_id,\n",
        "            \"filter[date]\": format_api_date(date),\n",
        "            \"filter[online_ticket]\": 0,\n",
        "            \"filter[is_promotion]\": 0,\n",
        "            \"filter[covid_utility]\": 0,\n",
        "            \"filter[speaking_english_utility]\": 0,\n",
        "            \"filter[enabled_gps]\": 0,\n",
        "            \"filter[has_cop]\": 0,\n",
        "            \"filter[online_reserved]\": 0,\n",
        "            \"filter[suggestion]\": \"DEFAULT\",\n",
        "            \"filter[fare][min]\": 0,\n",
        "            \"filter[fare][max]\": 2000000,\n",
        "            \"filter[available_seat][min]\": 1,\n",
        "            \"filter[available_seat][max]\": 50,\n",
        "            \"filter[rating][min]\": 0,\n",
        "            \"filter[rating][max]\": 5,\n",
        "            \"filter[limousine]\": 0,\n",
        "            \"filter[has_unfixed_point]\": 0,\n",
        "            \"page\": page,\n",
        "            \"pagesize\": pagesize\n",
        "        }\n",
        "\n",
        "        response = requests.get(api_url, headers=headers, params=query_params)\n",
        "\n",
        "        if response.status_code == 401:\n",
        "            print(f\"Token expired, please try again for {from_id} to {to_id} in {date}.\")\n",
        "            # get new token from token.txt file\n",
        "            raise ValueError(\"Token Expired\")\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to retrieve data for {date} on page {page}. Status Code: {response.status_code}\")\n",
        "            print(f\"Failed from {from_id} to {to_id}\")\n",
        "            break\n",
        "\n",
        "        data = response.json().get('data', [])\n",
        "        if not data:\n",
        "            # No more records to fetch\n",
        "            break\n",
        "        all_data.extend(data)\n",
        "        page += 1\n",
        "\n",
        "    return all_data\n",
        "\n",
        "\n",
        "# def process_bus_data(data):\n",
        "#     \"\"\"\n",
        "#     Process the API response data and count occurrences of each bus company.\n",
        "#     \"\"\"\n",
        "#     company_list = [route['company']['name'] for route in data if 'company' in route]\n",
        "#     return Counter(company_list)\n",
        "\n",
        "\n",
        "\n",
        "def process_bus_data(data):\n",
        "    \"\"\"\n",
        "    Process the API response data and, for each company, collect the unique available seat counts.\n",
        "\n",
        "    It assumes that each record in `data` contains a 'company' key and that the available seat count is\n",
        "    found in:\n",
        "        company['available_seat_info']['seat_type']['1']['total_available_seat']\n",
        "    as well as optionally in the 'seat_group' section.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary mapping each company name to a set of unique available seat counts.\n",
        "    \"\"\"\n",
        "    company_seat_counts = defaultdict(set)\n",
        "\n",
        "    for record in data:\n",
        "        company = record['company']['name']\n",
        "        if 'available_seat_info' not in record:\n",
        "            continue\n",
        "        seat_info = record['available_seat_info']['seat_type'].keys()\n",
        "        for key in seat_info:\n",
        "            seat_info = record['available_seat_info']['seat_type'][key]['total_available_seat']\n",
        "            company_seat_counts[company].add(seat_info)\n",
        "\n",
        "    seat_counts = {company: len(seats) for company, seats in company_seat_counts.items()}\n",
        "    return seat_counts\n",
        "\n",
        "\n",
        "def group_and_sum(dataframe):\n",
        "    \"\"\"\n",
        "    Groups the DataFrame by 'Company Name' and sums the 'Count' for each group.\n",
        "\n",
        "    Parameters:\n",
        "        dataframe (pd.DataFrame): The input DataFrame with columns 'Company Name' and 'Count'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with each company and the corresponding summed count.\n",
        "    \"\"\"\n",
        "    return dataframe.groupby('Company Name', as_index=False)['Count'].sum()\n",
        "\n",
        "\n",
        "def get_bus_count(token, from_destination, to_destination, dates=[\"27-02-2025\"], mapping_csv=\"mapping.csv\"):\n",
        "    \"\"\"\n",
        "    Get the number of bus trips from a source to a destination for multiple dates.\n",
        "    Saves the result as a CSV in the \"result\" directory.\n",
        "\n",
        "    :param token: API authentication token\n",
        "    :param from_destination: Departure city\n",
        "    :param to_destination: Destination city\n",
        "    :param dates: List of dates (default: [\"27-02-2025\"])\n",
        "    :param mapping_csv: Path to the city mapping CSV file\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    # Load city mapping file\n",
        "    city_df = pd.read_csv(mapping_csv)\n",
        "    from_id = get_city_id(city_df, from_destination)\n",
        "    to_id = get_city_id(city_df, to_destination)\n",
        "\n",
        "    # Ensure result directory exists\n",
        "    result_dir = \"result\"\n",
        "    os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "    all_data = []\n",
        "    total_count = 0\n",
        "\n",
        "    for date in dates:\n",
        "        data = fetch_bus_data(token, from_id, to_id, date)\n",
        "        company_counts = process_bus_data(data)\n",
        "        # print(company_counts)\n",
        "        if data == []:\n",
        "            # print(f\"No data available from {from_destination} to {to_destination} in {date}.\")\n",
        "            continue\n",
        "        if not company_counts:\n",
        "            # print(f\"No data available from {from_destination} to {to_destination} in {date}.\")\n",
        "            continue\n",
        "\n",
        "        df_counts = pd.DataFrame(sorted(company_counts.items(), key=lambda x: x[1], reverse=True),\n",
        "                                 columns=[\"Company Name\", \"Count\"])\n",
        "        total_count += df_counts[\"Count\"].sum()\n",
        "        df_counts['Date'] = f\"{date}\"\n",
        "        all_data.append(df_counts)\n",
        "    if all_data == []:\n",
        "        return None\n",
        "    # Merge all dates into one DataFrame\n",
        "    if all_data:\n",
        "        final_df = pd.concat(all_data, ignore_index=True)\n",
        "        total_row = pd.DataFrame([[\"Total\", total_count]], columns=[\"Company Name\", \"Count\", \"Date\"])\n",
        "        group_and_sum_df = group_and_sum(final_df)\n",
        "        group_and_sum_df['routes'] = f\"{from_destination} - {to_destination}\"\n",
        "        df_final = pd.concat([final_df, total_row], ignore_index=True)\n",
        "        # Save results to CSV\n",
        "        final_file = f\"{result_dir}/{from_destination}_{to_destination}_all_dates.csv\"\n",
        "        df_final.to_csv(final_file, index=False)\n",
        "        # print(f\"All data saved to {final_file}\")\n",
        "    else:\n",
        "        print(\"No data to save.\")\n",
        "\n",
        "    return from_destination, to_destination, total_count, group_and_sum_df\n",
        "\n",
        "\n",
        "def get_best(df, top = 20):\n",
        "    # Create a normalized route column by sorting the two destinations\n",
        "    df['route'] = df.apply(\n",
        "        lambda row: tuple(sorted([row['from destination'], row['to destination']])), axis=1\n",
        "    )\n",
        "    # For each route group, get the index of the row with the highest total count\n",
        "    idx = df.groupby('route')['total count'].idxmax()\n",
        "\n",
        "    # Get the unique routes with their maximum count\n",
        "    df_unique = df.loc[idx]\n",
        "\n",
        "    # Sort the results by 'total count' in descending order and take the top 20\n",
        "    top_20 = df_unique.sort_values('total count', ascending=False).head(top)\n",
        "\n",
        "    return top_20\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nDNneKULM-O-",
        "outputId": "1b625d6a-7360-4677-9ee7-b6fed4217ceb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing data: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Đà Nẵng\n",
            "Đà Nẵng\n",
            "Đà Nẵng\n",
            "Quảng Nam\n",
            "Token expired, please try again for 15 to 47 in 14-03-2025.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing data: 2it [00:01,  1.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quảng Nam\n",
            "Đà Nẵng\n",
            "[                       Company Name  Count        Date\n",
            "0      Hải Vân Limousine (xehue.vn)      3  14-03-2025\n",
            "1                 Cửa Ông Limousine      2  14-03-2025\n",
            "2          Tân Quang Dũng Limousine      2  14-03-2025\n",
            "3                Khang Vy Limousine      2  14-03-2025\n",
            "4               Ba Thanh Limousine       1  14-03-2025\n",
            "5   Hạnh Cafe - Hà Phương Limousine      1  14-03-2025\n",
            "6                          Phúc Lâm      1  14-03-2025\n",
            "7                      Gia Phát Car      1  14-03-2025\n",
            "8                       Trường Phát      1  14-03-2025\n",
            "9                        Hoàng Phúc      1  14-03-2025\n",
            "10                         Trâm Jet      1  14-03-2025\n",
            "11                 Barri Ann Travel      1  14-03-2025\n",
            "12                  VietNam Explore      1  14-03-2025\n",
            "13               Bảo Ngọc (Đà Nẵng)      1  14-03-2025\n",
            "14             Quốc Cường (Đà Nẵng)      1  14-03-2025]\n",
            "[                       Company Name  Count        Date\n",
            "0      Hải Vân Limousine (xehue.vn)      3  14-03-2025\n",
            "1                 Cửa Ông Limousine      2  14-03-2025\n",
            "2          Tân Quang Dũng Limousine      2  14-03-2025\n",
            "3                Khang Vy Limousine      2  14-03-2025\n",
            "4               Ba Thanh Limousine       1  14-03-2025\n",
            "5   Hạnh Cafe - Hà Phương Limousine      1  14-03-2025\n",
            "6                          Phúc Lâm      1  14-03-2025\n",
            "7                      Gia Phát Car      1  14-03-2025\n",
            "8                       Trường Phát      1  14-03-2025\n",
            "9                        Hoàng Phúc      1  14-03-2025\n",
            "10                         Trâm Jet      1  14-03-2025\n",
            "11                 Barri Ann Travel      1  14-03-2025\n",
            "12                  VietNam Explore      1  14-03-2025\n",
            "13               Bảo Ngọc (Đà Nẵng)      1  14-03-2025\n",
            "14             Quốc Cường (Đà Nẵng)      1  14-03-2025,                        Company Name  Count        Date\n",
            "0                  Barri Ann Travel      2  15-03-2025\n",
            "1                 Cửa Ông Limousine      2  15-03-2025\n",
            "2      Hải Vân Limousine (xehue.vn)      2  15-03-2025\n",
            "3                Khang Vy Limousine      2  15-03-2025\n",
            "4               Ba Thanh Limousine       1  15-03-2025\n",
            "5   Hạnh Cafe - Hà Phương Limousine      1  15-03-2025\n",
            "6                          Phúc Lâm      1  15-03-2025\n",
            "7                      Gia Phát Car      1  15-03-2025\n",
            "8                       Trường Phát      1  15-03-2025\n",
            "9                        Hoàng Phúc      1  15-03-2025\n",
            "10                         Trâm Jet      1  15-03-2025\n",
            "11                  VietNam Explore      1  15-03-2025\n",
            "12         Tân Quang Dũng Limousine      1  15-03-2025\n",
            "13               Bảo Ngọc (Đà Nẵng)      1  15-03-2025\n",
            "14             Quốc Cường (Đà Nẵng)      1  15-03-2025]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing data: 4it [00:09,  2.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quảng Nam\n",
            "Quảng Nam\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No objects to concatenate",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-fb2b1dd60231>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m     op = _Concatenator(\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_keys_and_objs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;31m# figure out what our result ndim is going to be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m_clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "\n",
        "def get_vexere_token(payload=None, headers=None):\n",
        "    url = \"https://vexere.com/getToken\"\n",
        "    # Default payload and headers (modify if necessary)\n",
        "    default_payload = {}  # Add required parameters if needed\n",
        "    default_headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            url,\n",
        "            json=payload or default_payload,\n",
        "            headers=headers or default_headers\n",
        "        )\n",
        "        response.raise_for_status()  # Raise an error for bad responses (4xx and 5xx)\n",
        "\n",
        "        data = response.json()  # Parse JSON response\n",
        "        access_token = data.get(\"access_token\")\n",
        "\n",
        "        if access_token:\n",
        "            return access_token\n",
        "        else:\n",
        "            raise ValueError(\"Access token not found in the response\")\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching token: {e}\")\n",
        "        return None\n",
        "\n",
        "city_df = pd.read_csv(\"mapping.csv\")\n",
        "city_list = [\"Đà Nẵng\", \"Quảng Nam\", \"Quảng Ngãi\", \"Bình Định\", \"Phú Yên\", \"Khánh Hòa\", \"Ninh Thuận\", \"Bình Thuận\", \"Kon Tum\", \"Gia Lai\", \"Đắk Lắk\", \"Đắk Nông\", \"Lâm Đồng\", \"Bình Phước\", \"Bình Dương\", \"Đồng Nai\", \"Tây Ninh\", \"Bà Rịa - Vũng Tàu\", \"Hồ Chí Minh\", \"Long An\", \"Tiền Giang\", \"Bến Tre\", \"Trà Vinh\", \"Vĩnh Long\", \"Đồng Tháp\", \"An Giang\", \"Kiên Giang\", \"Cần Thơ\", \"Hậu Giang\", \"Sóc Trăng\", \"Bạc Liêu\", \"Cà Mau\"]\n",
        "# city_list = city_df['name'].tolist()\n",
        "token = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ0eXAiOjIsInVzciI6ImZlIiwiY2lkIjoiYTRlYWM1MDAtMzYyNC0xMWU1LWFjOWUtMDkxMjRjNjAxMDEzIiwiZXhwIjoxNzQxNDIzNDU2fQ.PpWZmZ8QDRcURstaybMswydeojLRSG1ZRsQSjP6OxOs\"\n",
        "pairs = itertools.product(city_list[:2], repeat=2)\n",
        "date = [\"14-03-2025\", \"15-03-2025\"]\n",
        "# Print pairs, skipping (i, i) pairs\n",
        "results = []\n",
        "df = pd.DataFrame(columns=['from destination', 'to destination', 'total count'])\n",
        "for pair in tqdm(pairs, desc=\"Processing data\"):\n",
        "  print(pair[0])\n",
        "  print(pair[1])\n",
        "  try:\n",
        "    if pair[0] != pair[1]:\n",
        "      data_ = get_bus_count(token, pair[0], pair[1], date)\n",
        "      print(data_)\n",
        "      if data_ is not None:\n",
        "        a, b, c, d = data_\n",
        "      else:\n",
        "        continue\n",
        "      df.loc[len(df)] = [a, b, c]  # Inserts at the next available row index\n",
        "      results.append(d)\n",
        "\n",
        "  except ValueError as e:\n",
        "      token = get_vexere_token()\n",
        "      continue\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    continue\n",
        "# results = pd.concat(results, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYEy36uKM-O_",
        "outputId": "df59519b-f384-43fd-f88b-37aa5cdb0af3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Company Name  Count                 routes        date\n",
            "0     Ba Thanh Limousine       1    Đà Nẵng - Quảng Nam  15-03-2025\n",
            "1        Barri Ann Travel      1    Đà Nẵng - Quảng Nam  15-03-2025\n",
            "2      Bảo Ngọc (Đà Nẵng)      1    Đà Nẵng - Quảng Nam  15-03-2025\n",
            "3       Cửa Ông Limousine      2    Đà Nẵng - Quảng Nam  15-03-2025\n",
            "4     Diên Hồng (Đà Nẵng)      2    Đà Nẵng - Quảng Nam  15-03-2025\n",
            "...                   ...    ...                    ...         ...\n",
            "4259            Tuấn Hiệp      1  Bạc Liêu - Bình Phước  15-03-2025\n",
            "4260              Lê Hùng      1  Bạc Liêu - Bình Dương  15-03-2025\n",
            "4261            Tuấn Hiệp      1  Bạc Liêu - Bình Dương  15-03-2025\n",
            "4262            Tuấn Hiệp      4    Bạc Liêu - Đồng Nai  15-03-2025\n",
            "4263             Tân Niên      1    Bạc Liêu - Đồng Nai  15-03-2025\n",
            "\n",
            "[4264 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "# This is all the company name, number of count trip, for each date, for each route\n",
        "print(results)\n",
        "# Save the results to a CSV file\n",
        "results.to_csv(\"results.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "I-mC1Hnp3X5E",
        "outputId": "a7026e0d-43ee-4626-f603-a6de1e2f77c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/results.csv /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "Y6Z9VtEv3gn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-c_nhlUM-O_",
        "outputId": "6b3eb94c-3b81-47ba-dd52-99cff85b61c0"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'openpyxl'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      5\u001b[39m sort_by_count = group_by_company.sort_values(\u001b[33m'\u001b[39m\u001b[33mCount\u001b[39m\u001b[33m'\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m).head(\u001b[32m20\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Save the result to a CSV file\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# save_file = f\"result/group_by_company.csv\"\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# group_by_company.to_csv(save_file, index=False)\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Save the result to a XLXS file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43msort_by_count\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresult/group_by_company.xlsx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/carrier_data_scrapping/.venv/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/carrier_data_scrapping/.venv/lib/python3.12/site-packages/pandas/core/generic.py:2417\u001b[39m, in \u001b[36mNDFrame.to_excel\u001b[39m\u001b[34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   2404\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mformats\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexcel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[32m   2406\u001b[39m formatter = ExcelFormatter(\n\u001b[32m   2407\u001b[39m     df,\n\u001b[32m   2408\u001b[39m     na_rep=na_rep,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2415\u001b[39m     inf_rep=inf_rep,\n\u001b[32m   2416\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2417\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2419\u001b[39m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2420\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2422\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2423\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2424\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2425\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2426\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/carrier_data_scrapping/.venv/lib/python3.12/site-packages/pandas/io/formats/excel.py:943\u001b[39m, in \u001b[36mExcelFormatter.write\u001b[39m\u001b[34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m    941\u001b[39m     need_save = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     writer = \u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    949\u001b[39m     need_save = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/carrier_data_scrapping/.venv/lib/python3.12/site-packages/pandas/io/excel/_openpyxl.py:57\u001b[39m, in \u001b[36mOpenpyxlWriter.__init__\u001b[39m\u001b[34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     46\u001b[39m     path: FilePath | WriteExcelBuffer | ExcelWriter,\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     56\u001b[39m     \u001b[38;5;66;03m# Use the openpyxl module as the Excel writer.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenpyxl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mworkbook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[32m     59\u001b[39m     engine_kwargs = combine_kwargs(engine_kwargs, kwargs)\n\u001b[32m     61\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m     62\u001b[39m         path,\n\u001b[32m     63\u001b[39m         mode=mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m         engine_kwargs=engine_kwargs,\n\u001b[32m     67\u001b[39m     )\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'openpyxl'"
          ]
        }
      ],
      "source": [
        "# This is group by company name, sum the number of count trip, for all date, for all route\n",
        "# If you want to group by company name, delete the 'routes' in the list group_by_condition\n",
        "group_by_condition = ['Company Name']\n",
        "group_by_company = results.groupby(group_by_condition, as_index=False)['Count'].sum()\n",
        "sort_by_count = group_by_company.sort_values('Count', ascending=False).head(20)\n",
        "# Save the result to a CSV file\n",
        "# save_file = f\"result/group_by_company.csv\"\n",
        "# group_by_company.to_csv(save_file, index=False)\n",
        "# Save the result to a XLXS file\n",
        "sort_by_count.to_excel(\"result/group_by_company.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7g62nIxKM-PA",
        "outputId": "56589322-e235-4127-a937-aab6be7f20e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  from destination to destination  total count                    route\n",
            "0          Đà Nẵng      Quảng Nam           27     (Quảng Nam, Đà Nẵng)\n",
            "1          Đà Nẵng     Quảng Ngãi           26    (Quảng Ngãi, Đà Nẵng)\n",
            "3        Quảng Nam     Quảng Ngãi           17  (Quảng Nam, Quảng Ngãi)\n"
          ]
        }
      ],
      "source": [
        "# This return top best 20, 50 number of route with the highest number of count trip\n",
        "# If you want to change the number of top, change the 'top' in the function get_best\n",
        "top_best = get_best(df, top = 20)\n",
        "print(top_best)\n",
        "# Save the result to a CSV file\n",
        "# save_file = f\"result/top_best.csv\"\n",
        "# top_best.to_csv(save_file, index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}