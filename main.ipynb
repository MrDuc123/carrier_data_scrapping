{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2cTircjM-O7",
        "outputId": "cc5fd02a-f65a-4616-a901-7b4ec937969a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching data: 100%|██████████| 69/69 [01:16<00:00,  1.11s/it]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "\n",
        "def slugify(text):\n",
        "    \"\"\"\n",
        "    Convert a string to a slug by:\n",
        "    - Removing diacritics (e.g., converting 'Đà Nẵng' to 'Da Nang')\n",
        "    - Lowercasing\n",
        "    - Replacing non-alphanumeric characters with hyphens\n",
        "    \"\"\"\n",
        "    # Normalize to remove accents\n",
        "    # Replace special characters Đ and đ with D and d\n",
        "    text = text.replace('Đ', 'D').replace('đ', 'd')\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
        "    text = text.lower()\n",
        "    # Replace any non-alphanumeric characters with hyphens and remove extra hyphens\n",
        "    text = re.sub(r'[^a-z0-9]+', '-', text).strip('-')\n",
        "    return text\n",
        "\n",
        "\n",
        "# Function to fetch data from the API\n",
        "def fetch_data(token, type_list, range_start, range_end):\n",
        "    results = []  # List to store each record as a dictionary\n",
        "    url = \"https://internal-vroute-cmc.vexere.com/v1/goyolo/area/\"\n",
        "    # Provided Bearer token\n",
        "\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\"\n",
        "    }\n",
        "    for i in tqdm(range(range_start,range_end), desc=\"Fetching data\"):\n",
        "        name = None\n",
        "        slug = None\n",
        "        url = f\"https://internal-vroute-cmc.vexere.com/v1/goyolo/area/{str(i)}\"\n",
        "        response = requests.get(url, headers=headers)\n",
        "        # Check if the HTTP request was successful\n",
        "        if response.status_code == 401:\n",
        "            print(\"Token expired\")\n",
        "            return None\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            if data['data']['type'] not in type_list:\n",
        "                continue\n",
        "            # Check if the returned message is \"success\"\n",
        "            if data.get(\"message\") == \"success\":\n",
        "              try:\n",
        "                # Extract and print the name from the \"data\" field\n",
        "                name = data.get(\"data\", {}).get(\"name\")\n",
        "                # Create a slug from the name, if available.\n",
        "                slug = slugify(name)\n",
        "                results.append({\"id\": i, \"name\": name, \"slug\": slug})\n",
        "              except Exception as e:\n",
        "                continue\n",
        "    # Convert the list of dictionaries to a DataFrame\n",
        "    a = pd.DataFrame(results)\n",
        "    a.to_csv(\"mapping.csv\", index=False)\n",
        "\n",
        "    return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    token =\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ0eXAiOjIsInVzciI6ImZlIiwiY2lkIjoiYTRlYWM1MDAtMzYyNC0xMWU1LWFjOWUtMDkxMjRjNjAxMDEzIiwiZXhwIjoxNzQxMzY4ODYyfQ.J6DuAYjL4o9XxA2eHVib5xet7luU7QFzN3Sa_HgU1XU\"\n",
        "    type_list = [3]\n",
        "    fetch_data(token, type_list, 1, 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd-qkvtgM-O9"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "def slugify(text):\n",
        "    \"\"\"\n",
        "    Convert a string to a slug:\n",
        "    - Remove diacritics (e.g., 'Đà Nẵng' → 'Da Nang')\n",
        "    - Convert to lowercase\n",
        "    - Replace non-alphanumeric characters with hyphens\n",
        "    \"\"\"\n",
        "    text = text.replace('Đ', 'D').replace('đ', 'd')\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
        "    text = text.lower()\n",
        "    return re.sub(r'[^a-z0-9]+', '-', text).strip('-')\n",
        "\n",
        "\n",
        "def format_api_date(date_str):\n",
        "    \"\"\"\n",
        "    Convert a date from 'DD-MM-YYYY' format to the API format: 'YYYY-MM-DDT00:00:00+07:00'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        dt = datetime.strptime(date_str, \"%d-%m-%Y\")\n",
        "        return dt.strftime(\"%Y-%m-%dT00:00:00+07:00\")\n",
        "    except ValueError:\n",
        "        raise ValueError(\"Invalid date format. Use 'DD-MM-YYYY'.\")\n",
        "\n",
        "\n",
        "def get_bus_trip_count(url):\n",
        "\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(\"Failed to retrieve the page.\")\n",
        "        return None\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Find the element with class \"text-result-number\"\n",
        "    result_element = soup.find(class_=\"text-result-number\")\n",
        "\n",
        "    if result_element:\n",
        "        # Get the text and strip any surrounding whitespace\n",
        "        count_text = result_element.get_text(strip=True)\n",
        "        count_text = count_text.split()\n",
        "        return count_text[0]\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def get_total(from_destination, to_destination, date=\"27-02-2025\", mapping_csv=\"mapping.csv\"):\n",
        "    \"\"\"\n",
        "    Given departure and destination names, enrich the data by:\n",
        "      - Reading the mapping CSV (which has columns: id, name, slug),\n",
        "      - Normalizing the destination names using slugify,\n",
        "      - Building the route code and URL,\n",
        "      - Fetching the total bus trip count from the URL.\n",
        "\n",
        "    Returns a DataFrame with columns: from_name, to_name, total_trip, date.\n",
        "    \"\"\"\n",
        "    # Read the mapping CSV into a DataFrame.\n",
        "    city_df = pd.read_csv(mapping_csv)\n",
        "\n",
        "    # Normalize the destination names using slugify.\n",
        "    normalized_from = slugify(from_destination)\n",
        "    normalized_to = slugify(to_destination)\n",
        "\n",
        "    # Look up the corresponding rows in the mapping CSV.\n",
        "    row_from = city_df[city_df['slug'] == normalized_from]\n",
        "    row_to = city_df[city_df['slug'] == normalized_to]\n",
        "\n",
        "    if row_from.empty or row_to.empty:\n",
        "        print(\"Error: One or both destinations not found in mapping CSV.\")\n",
        "        return None\n",
        "\n",
        "    # Extract required fields.\n",
        "    from_id = row_from.iloc[0]['id']\n",
        "    from_slug = row_from.iloc[0]['slug']\n",
        "    to_id = row_to.iloc[0]['id']\n",
        "    to_slug = row_to.iloc[0]['slug']\n",
        "\n",
        "    # Generate the route code.\n",
        "    route_code = f\"1{from_id}t1{to_id}1\"\n",
        "\n",
        "    # Build the URL.\n",
        "    url = (\n",
        "        f\"https://vexere.com/vi-VN/ve-xe-khach-tu-{from_slug}-di-{to_slug}-\"\n",
        "        f\"{route_code}.html?date={date}\"\n",
        "    )\n",
        "\n",
        "    # Fetch the total trip count using the provided function.\n",
        "    total_trip = get_bus_trip_count(url)\n",
        "\n",
        "    return total_trip\n",
        "\n",
        "\n",
        "\n",
        "def get_city_id(city_df, city_name):\n",
        "    \"\"\"\n",
        "    Retrieve the city ID from the mapping CSV based on the city name.\n",
        "    \"\"\"\n",
        "    city_row = city_df[city_df['slug'] == slugify(city_name)]\n",
        "    if not city_row.empty:\n",
        "        return city_row.iloc[0, 0]  # Assuming city ID is in the first column\n",
        "    raise ValueError(f\"City '{city_name}' not found in the mapping file.\")\n",
        "\n",
        "\n",
        "\n",
        "def fetch_bus_data(token, from_id, to_id, date):\n",
        "    \"\"\"\n",
        "    Fetch bus data from the API for a given route and date.\n",
        "    Implements pagination to retrieve more than 100 records.\n",
        "    \"\"\"\n",
        "    api_url = \"https://internal-vroute-cmc.vexere.com/v2/route\"\n",
        "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "\n",
        "    all_data = []\n",
        "    page = 1\n",
        "    pagesize = 100000  # adjust if needed\n",
        "    while True:\n",
        "        query_params = {\n",
        "            \"filter[from]\": from_id,\n",
        "            \"filter[to]\": to_id,\n",
        "            \"filter[date]\": format_api_date(date),\n",
        "            \"filter[online_ticket]\": 0,\n",
        "            \"filter[is_promotion]\": 0,\n",
        "            \"filter[covid_utility]\": 0,\n",
        "            \"filter[speaking_english_utility]\": 0,\n",
        "            \"filter[enabled_gps]\": 0,\n",
        "            \"filter[has_cop]\": 0,\n",
        "            \"filter[online_reserved]\": 0,\n",
        "            \"filter[suggestion]\": \"DEFAULT\",\n",
        "            \"filter[fare][min]\": 0,\n",
        "            \"filter[fare][max]\": 2000000,\n",
        "            \"filter[available_seat][min]\": 1,\n",
        "            \"filter[available_seat][max]\": 50,\n",
        "            \"filter[rating][min]\": 0,\n",
        "            \"filter[rating][max]\": 5,\n",
        "            \"filter[limousine]\": 0,\n",
        "            \"filter[has_unfixed_point]\": 0,\n",
        "            \"page\": page,\n",
        "            \"pagesize\": pagesize\n",
        "        }\n",
        "\n",
        "        response = requests.get(api_url, headers=headers, params=query_params)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to retrieve data for {date} on page {page}. Status Code: {response.status_code}\")\n",
        "            print(json.dumps(response))\n",
        "            print(f\"Failed from {from_id} to {to_id}\")\n",
        "            break\n",
        "        \n",
        "        data = response.json().get('data', [])\n",
        "        if not data:\n",
        "            # No more records to fetch\n",
        "            break\n",
        "        if response.status_code == 401:\n",
        "            print(f\"Token expired, please try again for {from_id} to {to_id} in {date}.\")\n",
        "            # get new token from token.txt file\n",
        "            print(\"Waiting for 60 seconds to get new token\")\n",
        "            time.sleep(60)\n",
        "            with open(\"token.txt\", \"r\") as file:\n",
        "                token = file.read().strip()\n",
        "            continue\n",
        "             \n",
        "\n",
        "        all_data.extend(data)\n",
        "        page += 1\n",
        "\n",
        "    return all_data\n",
        "\n",
        "\n",
        "# def process_bus_data(data):\n",
        "#     \"\"\"\n",
        "#     Process the API response data and count occurrences of each bus company.\n",
        "#     \"\"\"\n",
        "#     company_list = [route['company']['name'] for route in data if 'company' in route]\n",
        "#     return Counter(company_list)\n",
        "\n",
        "\n",
        "\n",
        "def process_bus_data(data):\n",
        "    \"\"\"\n",
        "    Process the API response data and, for each company, collect the unique available seat counts.\n",
        "\n",
        "    It assumes that each record in `data` contains a 'company' key and that the available seat count is\n",
        "    found in:\n",
        "        company['available_seat_info']['seat_type']['1']['total_available_seat']\n",
        "    as well as optionally in the 'seat_group' section.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary mapping each company name to a set of unique available seat counts.\n",
        "    \"\"\"\n",
        "    company_seat_counts = defaultdict(set)\n",
        "\n",
        "    for record in data:\n",
        "        company = record['company']['name']\n",
        "        if 'available_seat_info' not in record:\n",
        "            continue\n",
        "        seat_info = record['available_seat_info']['seat_type'].keys()\n",
        "        for key in seat_info:\n",
        "            seat_info = record['available_seat_info']['seat_type'][key]['total_available_seat']\n",
        "            company_seat_counts[company].add(seat_info)\n",
        "\n",
        "    seat_counts = {company: len(seats) for company, seats in company_seat_counts.items()}\n",
        "    return seat_counts\n",
        "\n",
        "\n",
        "def group_and_sum(dataframe):\n",
        "    \"\"\"\n",
        "    Groups the DataFrame by 'Company Name' and sums the 'Count' for each group.\n",
        "\n",
        "    Parameters:\n",
        "        dataframe (pd.DataFrame): The input DataFrame with columns 'Company Name' and 'Count'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with each company and the corresponding summed count.\n",
        "    \"\"\"\n",
        "    return dataframe.groupby('Company Name', as_index=False)['Count'].sum()\n",
        "\n",
        "\n",
        "def get_bus_count(token, from_destination, to_destination, dates=[\"27-02-2025\"], mapping_csv=\"mapping.csv\"):\n",
        "    \"\"\"\n",
        "    Get the number of bus trips from a source to a destination for multiple dates.\n",
        "    Saves the result as a CSV in the \"result\" directory.\n",
        "\n",
        "    :param token: API authentication token\n",
        "    :param from_destination: Departure city\n",
        "    :param to_destination: Destination city\n",
        "    :param dates: List of dates (default: [\"27-02-2025\"])\n",
        "    :param mapping_csv: Path to the city mapping CSV file\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    # Load city mapping file\n",
        "    try:\n",
        "        city_df = pd.read_csv(mapping_csv)\n",
        "        from_id = get_city_id(city_df, from_destination)\n",
        "        to_id = get_city_id(city_df, to_destination)\n",
        "    except (FileNotFoundError, ValueError) as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Ensure result directory exists\n",
        "    result_dir = \"result\"\n",
        "    os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "    all_data = []\n",
        "    total_count = 0\n",
        "\n",
        "    for date in dates:\n",
        "        data = fetch_bus_data(token, from_id, to_id, date)\n",
        "        company_counts = process_bus_data(data)\n",
        "        # print(company_counts)\n",
        "        if data == []:\n",
        "            # print(f\"No data available from {from_destination} to {to_destination} in {date}.\")\n",
        "            continue\n",
        "        if not company_counts:\n",
        "            # print(f\"No data available from {from_destination} to {to_destination} in {date}.\")\n",
        "            continue\n",
        "\n",
        "        df_counts = pd.DataFrame(sorted(company_counts.items(), key=lambda x: x[1], reverse=True),\n",
        "                                 columns=[\"Company Name\", \"Count\"])\n",
        "        total_count += df_counts[\"Count\"].sum()\n",
        "        all_data.append(df_counts)\n",
        "\n",
        "    if all_data == []:\n",
        "        return None\n",
        "    # Merge all dates into one DataFrame\n",
        "    if all_data:\n",
        "        final_df = pd.concat(all_data, ignore_index=True)\n",
        "        total_row = pd.DataFrame([[\"Total\", total_count]], columns=[\"Company Name\", \"Count\"])\n",
        "        group_and_sum_df = group_and_sum(final_df)\n",
        "        group_and_sum_df['routes'] = f\"{from_destination} - {to_destination}\"\n",
        "        group_and_sum_df['date'] = f\"{date}\"\n",
        "\n",
        "        df_final = pd.concat([final_df, total_row], ignore_index=True)\n",
        "        # Save results to CSV\n",
        "        final_file = f\"{result_dir}/{from_destination}_{to_destination}_all_dates.csv\"\n",
        "        df_final.to_csv(final_file, index=False)\n",
        "        # print(f\"All data saved to {final_file}\")\n",
        "    else:\n",
        "        print(\"No data to save.\")\n",
        "\n",
        "    return from_destination, to_destination, total_count, group_and_sum_df\n",
        "\n",
        "\n",
        "def get_best(df, top = 20):\n",
        "    # Create a normalized route column by sorting the two destinations\n",
        "    df['route'] = df.apply(\n",
        "        lambda row: tuple(sorted([row['from destination'], row['to destination']])), axis=1\n",
        "    )\n",
        "    # For each route group, get the index of the row with the highest total count\n",
        "    idx = df.groupby('route')['total count'].idxmax()\n",
        "\n",
        "    # Get the unique routes with their maximum count\n",
        "    df_unique = df.loc[idx]\n",
        "\n",
        "    # Sort the results by 'total count' in descending order and take the top 20\n",
        "    top_20 = df_unique.sort_values('total count', ascending=False).head(top)\n",
        "\n",
        "    return top_20\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDNneKULM-O-",
        "outputId": "e760c797-b365-41ec-bc86-3e963d630059"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing data: 109it [04:47,  2.64s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     14\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m pair[\u001b[32m0\u001b[39m] != pair[\u001b[32m1\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     data_ = \u001b[43mget_bus_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpair\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     17\u001b[39m       a, b, c, d = data_\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 255\u001b[39m, in \u001b[36mget_bus_count\u001b[39m\u001b[34m(token, from_destination, to_destination, dates, mapping_csv)\u001b[39m\n\u001b[32m    252\u001b[39m total_count = \u001b[32m0\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m date \u001b[38;5;129;01min\u001b[39;00m dates:\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m     data = \u001b[43mfetch_bus_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m     company_counts = process_bus_data(data)\n\u001b[32m    257\u001b[39m     \u001b[38;5;66;03m# print(company_counts)\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 152\u001b[39m, in \u001b[36mfetch_bus_data\u001b[39m\u001b[34m(token, from_id, to_id, date)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    128\u001b[39m     query_params = {\n\u001b[32m    129\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfilter[from]\u001b[39m\u001b[33m\"\u001b[39m: from_id,\n\u001b[32m    130\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfilter[to]\u001b[39m\u001b[33m\"\u001b[39m: to_id,\n\u001b[32m   (...)\u001b[39m\u001b[32m    149\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpagesize\u001b[39m\u001b[33m\"\u001b[39m: pagesize\n\u001b[32m    150\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m    154\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to retrieve data for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m on page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Status Code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/carrier_data_scrapping/.venv/lib/python3.12/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/carrier_data_scrapping/.venv/lib/python3.12/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/carrier_data_scrapping/.venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/carrier_data_scrapping/.venv/lib/python3.12/site-packages/requests/sessions.py:746\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    743\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/carrier_data_scrapping/.venv/lib/python3.12/site-packages/requests/models.py:902\u001b[39m, in \u001b[36mResponse.content\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    900\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28mself\u001b[39m._content_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/carrier_data_scrapping/.venv/lib/python3.12/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/carrier_data_scrapping/.venv/lib/python3.12/site-packages/urllib3/response.py:1063\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1047\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m   1049\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1060\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m   1061\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1062\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1064\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/carrier_data_scrapping/.venv/lib/python3.12/site-packages/urllib3/response.py:1219\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1216\u001b[39m     amt = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1218\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1219\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1221\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/carrier_data_scrapping/.venv/lib/python3.12/site-packages/urllib3/response.py:1138\u001b[39m, in \u001b[36mHTTPResponse._update_chunk_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m line = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1139\u001b[39m line = line.split(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m   1140\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ssl.py:1252\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1250\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1251\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ssl.py:1104\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1105\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1106\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "\n",
        "city_df = pd.read_csv(\"mapping.csv\")\n",
        "city_list = [\"Đà Nẵng\", \"Quảng Nam\", \"Quảng Ngãi\", \"Bình Định\", \"Phú Yên\", \"Khánh Hòa\", \"Ninh Thuận\", \"Bình Thuận\", \"Kon Tum\", \"Gia Lai\", \"Đắk Lắk\", \"Đắk Nông\", \"Lâm Đồng\", \"Bình Phước\", \"Bình Dương\", \"Đồng Nai\", \"Tây Ninh\", \"Bà Rịa - Vũng Tàu\", \"Hồ Chí Minh\", \"Long An\", \"Tiền Giang\", \"Bến Tre\", \"Trà Vinh\", \"Vĩnh Long\", \"Đồng Tháp\", \"An Giang\", \"Kiên Giang\", \"Cần Thơ\", \"Hậu Giang\", \"Sóc Trăng\", \"Bạc Liêu\", \"Cà Mau\"]\n",
        "# city_list = city_df['name'].tolist()\n",
        "pairs = itertools.product(city_list, repeat=2)\n",
        "date = [\"15-03-2025\"]\n",
        "token =\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ0eXAiOjIsInVzciI6ImZlIiwiY2lkIjoiYTRlYWM1MDAtMzYyNC0xMWU1LWFjOWUtMDkxMjRjNjAxMDEzIiwiZXhwIjoxNzQxMzY4ODYyfQ.J6DuAYjL4o9XxA2eHVib5xet7luU7QFzN3Sa_HgU1XU\"\n",
        "# Print pairs, skipping (i, i) pairs\n",
        "results = []\n",
        "df = pd.DataFrame(columns=['from destination', 'to destination', 'total count'])\n",
        "for pair in tqdm(pairs, desc=\"Processing data\"):\n",
        "  try:\n",
        "    if pair[0] != pair[1]:\n",
        "      data_ = get_bus_count(token,pair[0], pair[1], date)\n",
        "      if data_ is not None:\n",
        "        a, b, c, d = data_\n",
        "      else:\n",
        "        continue\n",
        "      df.loc[len(df)] = [a, b, c]  # Inserts at the next available row index\n",
        "      results.append(d)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    continue\n",
        "results = pd.concat(results, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYEy36uKM-O_",
        "outputId": "09940d34-db32-4074-aa77-af1318c5d288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Company Name  Count                      routes        date\n",
            "0    Tuấn Hiệp      3  Bà Rịa-Vũng Tàu - Bạc Liêu  08-03-2025\n",
            "1    Tuấn Hiệp      3  Bạc Liêu - Bà Rịa-Vũng Tàu  08-03-2025\n"
          ]
        }
      ],
      "source": [
        "# This is all the company name, number of count trip, for each date, for each route\n",
        "print(results)\n",
        "# Save the results to a CSV file\n",
        "# results.to_csv(\"results.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-c_nhlUM-O_",
        "outputId": "6b3eb94c-3b81-47ba-dd52-99cff85b61c0"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'openpyxl'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      5\u001b[39m sort_by_count = group_by_company.sort_values(\u001b[33m'\u001b[39m\u001b[33mCount\u001b[39m\u001b[33m'\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m).head(\u001b[32m20\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Save the result to a CSV file\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# save_file = f\"result/group_by_company.csv\"\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# group_by_company.to_csv(save_file, index=False)\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Save the result to a XLXS file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43msort_by_count\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresult/group_by_company.xlsx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/carrier_data_scrapping/.venv/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/carrier_data_scrapping/.venv/lib/python3.12/site-packages/pandas/core/generic.py:2417\u001b[39m, in \u001b[36mNDFrame.to_excel\u001b[39m\u001b[34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   2404\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mformats\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexcel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[32m   2406\u001b[39m formatter = ExcelFormatter(\n\u001b[32m   2407\u001b[39m     df,\n\u001b[32m   2408\u001b[39m     na_rep=na_rep,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2415\u001b[39m     inf_rep=inf_rep,\n\u001b[32m   2416\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2417\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2419\u001b[39m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2420\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2422\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2423\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2424\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2425\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2426\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/carrier_data_scrapping/.venv/lib/python3.12/site-packages/pandas/io/formats/excel.py:943\u001b[39m, in \u001b[36mExcelFormatter.write\u001b[39m\u001b[34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m    941\u001b[39m     need_save = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     writer = \u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    949\u001b[39m     need_save = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/carrier_data_scrapping/.venv/lib/python3.12/site-packages/pandas/io/excel/_openpyxl.py:57\u001b[39m, in \u001b[36mOpenpyxlWriter.__init__\u001b[39m\u001b[34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     46\u001b[39m     path: FilePath | WriteExcelBuffer | ExcelWriter,\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     56\u001b[39m     \u001b[38;5;66;03m# Use the openpyxl module as the Excel writer.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenpyxl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mworkbook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[32m     59\u001b[39m     engine_kwargs = combine_kwargs(engine_kwargs, kwargs)\n\u001b[32m     61\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m     62\u001b[39m         path,\n\u001b[32m     63\u001b[39m         mode=mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m         engine_kwargs=engine_kwargs,\n\u001b[32m     67\u001b[39m     )\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'openpyxl'"
          ]
        }
      ],
      "source": [
        "# This is group by company name, sum the number of count trip, for all date, for all route\n",
        "# If you want to group by company name, delete the 'routes' in the list group_by_condition\n",
        "group_by_condition = ['Company Name']\n",
        "group_by_company = results.groupby(group_by_condition, as_index=False)['Count'].sum()\n",
        "sort_by_count = group_by_company.sort_values('Count', ascending=False).head(20)\n",
        "# Save the result to a CSV file\n",
        "# save_file = f\"result/group_by_company.csv\"\n",
        "# group_by_company.to_csv(save_file, index=False)\n",
        "# Save the result to a XLXS file\n",
        "sort_by_count.to_excel(\"result/group_by_company.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7g62nIxKM-PA",
        "outputId": "56589322-e235-4127-a937-aab6be7f20e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  from destination to destination  total count                    route\n",
            "0          Đà Nẵng      Quảng Nam           27     (Quảng Nam, Đà Nẵng)\n",
            "1          Đà Nẵng     Quảng Ngãi           26    (Quảng Ngãi, Đà Nẵng)\n",
            "3        Quảng Nam     Quảng Ngãi           17  (Quảng Nam, Quảng Ngãi)\n"
          ]
        }
      ],
      "source": [
        "# This return top best 20, 50 number of route with the highest number of count trip\n",
        "# If you want to change the number of top, change the 'top' in the function get_best\n",
        "top_best = get_best(df, top = 20)\n",
        "print(top_best)\n",
        "# Save the result to a CSV file\n",
        "# save_file = f\"result/top_best.csv\"\n",
        "# top_best.to_csv(save_file, index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
