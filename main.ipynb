{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2cTircjM-O7",
        "outputId": "cc5fd02a-f65a-4616-a901-7b4ec937969a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching data: 100%|██████████| 69/69 [01:07<00:00,  1.02it/s]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "\n",
        "def slugify(text):\n",
        "    \"\"\"\n",
        "    Convert a string to a slug by:\n",
        "    - Removing diacritics (e.g., converting 'Đà Nẵng' to 'Da Nang')\n",
        "    - Lowercasing\n",
        "    - Replacing non-alphanumeric characters with hyphens\n",
        "    \"\"\"\n",
        "    # Normalize to remove accents\n",
        "    # Replace special characters Đ and đ with D and d\n",
        "    text = text.replace('Đ', 'D').replace('đ', 'd')\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
        "    text = text.lower()\n",
        "    # Replace any non-alphanumeric characters with hyphens and remove extra hyphens\n",
        "    text = re.sub(r'[^a-z0-9]+', '-', text).strip('-')\n",
        "    return text\n",
        "\n",
        "\n",
        "# Function to fetch data from the API\n",
        "def fetch_data(token, type_list, range_start, range_end):\n",
        "    results = []  # List to store each record as a dictionary\n",
        "    url = \"https://internal-vroute-cmc.vexere.com/v1/goyolo/area/\"\n",
        "    # Provided Bearer token\n",
        "\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\"\n",
        "    }\n",
        "    for i in tqdm(range(range_start,range_end), desc=\"Fetching data\"):\n",
        "        name = None\n",
        "        slug = None\n",
        "        url = f\"https://internal-vroute-cmc.vexere.com/v1/goyolo/area/{str(i)}\"\n",
        "        response = requests.get(url, headers=headers)\n",
        "        # Check if the HTTP request was successful\n",
        "        if response.status_code == 401:\n",
        "            print(\"Token expired\")\n",
        "            return None\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            if data['data']['type'] not in type_list:\n",
        "                continue\n",
        "            # Check if the returned message is \"success\"\n",
        "            if data.get(\"message\") == \"success\":\n",
        "              try:\n",
        "                # Extract and print the name from the \"data\" field\n",
        "                name = data.get(\"data\", {}).get(\"name\")\n",
        "                # Create a slug from the name, if available.\n",
        "                slug = slugify(name)\n",
        "                results.append({\"id\": i, \"name\": name, \"slug\": slug})\n",
        "              except Exception as e:\n",
        "                continue\n",
        "    # Convert the list of dictionaries to a DataFrame\n",
        "    a = pd.DataFrame(results)\n",
        "    a.to_csv(\"mapping.csv\", index=False)\n",
        "\n",
        "    return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    token = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ0eXAiOjIsInVzciI6ImZlIiwiY2lkIjoiYTRlYWM1MDAtMzYyNC0xMWU1LWFjOWUtMDkxMjRjNjAxMDEzIiwiZXhwIjoxNzQxMzYzODk0fQ.HuItYy9aH6txxwsOmG63IQbrp8keO-keBrQmOOb9TVw\"\n",
        "    type_list = [3]\n",
        "    fetch_data(token, type_list, 1, 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd-qkvtgM-O9"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import re\n",
        "import os\n",
        "\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "def slugify(text):\n",
        "    \"\"\"\n",
        "    Convert a string to a slug:\n",
        "    - Remove diacritics (e.g., 'Đà Nẵng' → 'Da Nang')\n",
        "    - Convert to lowercase\n",
        "    - Replace non-alphanumeric characters with hyphens\n",
        "    \"\"\"\n",
        "    text = text.replace('Đ', 'D').replace('đ', 'd')\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
        "    text = text.lower()\n",
        "    return re.sub(r'[^a-z0-9]+', '-', text).strip('-')\n",
        "\n",
        "\n",
        "def format_api_date(date_str):\n",
        "    \"\"\"\n",
        "    Convert a date from 'DD-MM-YYYY' format to the API format: 'YYYY-MM-DDT00:00:00+07:00'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        dt = datetime.strptime(date_str, \"%d-%m-%Y\")\n",
        "        return dt.strftime(\"%Y-%m-%dT00:00:00+07:00\")\n",
        "    except ValueError:\n",
        "        raise ValueError(\"Invalid date format. Use 'DD-MM-YYYY'.\")\n",
        "\n",
        "\n",
        "def get_bus_trip_count(url):\n",
        "\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(\"Failed to retrieve the page.\")\n",
        "        return None\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Find the element with class \"text-result-number\"\n",
        "    result_element = soup.find(class_=\"text-result-number\")\n",
        "\n",
        "    if result_element:\n",
        "        # Get the text and strip any surrounding whitespace\n",
        "        count_text = result_element.get_text(strip=True)\n",
        "        count_text = count_text.split()\n",
        "        return count_text[0]\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def get_total(from_destination, to_destination, date=\"27-02-2025\", mapping_csv=\"mapping.csv\"):\n",
        "    \"\"\"\n",
        "    Given departure and destination names, enrich the data by:\n",
        "      - Reading the mapping CSV (which has columns: id, name, slug),\n",
        "      - Normalizing the destination names using slugify,\n",
        "      - Building the route code and URL,\n",
        "      - Fetching the total bus trip count from the URL.\n",
        "\n",
        "    Returns a DataFrame with columns: from_name, to_name, total_trip, date.\n",
        "    \"\"\"\n",
        "    # Read the mapping CSV into a DataFrame.\n",
        "    city_df = pd.read_csv(mapping_csv)\n",
        "\n",
        "    # Normalize the destination names using slugify.\n",
        "    normalized_from = slugify(from_destination)\n",
        "    normalized_to = slugify(to_destination)\n",
        "\n",
        "    # Look up the corresponding rows in the mapping CSV.\n",
        "    row_from = city_df[city_df['slug'] == normalized_from]\n",
        "    row_to = city_df[city_df['slug'] == normalized_to]\n",
        "\n",
        "    if row_from.empty or row_to.empty:\n",
        "        print(\"Error: One or both destinations not found in mapping CSV.\")\n",
        "        return None\n",
        "\n",
        "    # Extract required fields.\n",
        "    from_id = row_from.iloc[0]['id']\n",
        "    from_slug = row_from.iloc[0]['slug']\n",
        "    to_id = row_to.iloc[0]['id']\n",
        "    to_slug = row_to.iloc[0]['slug']\n",
        "\n",
        "    # Generate the route code.\n",
        "    route_code = f\"1{from_id}t1{to_id}1\"\n",
        "\n",
        "    # Build the URL.\n",
        "    url = (\n",
        "        f\"https://vexere.com/vi-VN/ve-xe-khach-tu-{from_slug}-di-{to_slug}-\"\n",
        "        f\"{route_code}.html?date={date}\"\n",
        "    )\n",
        "\n",
        "    # Fetch the total trip count using the provided function.\n",
        "    total_trip = get_bus_trip_count(url)\n",
        "\n",
        "    return total_trip\n",
        "\n",
        "\n",
        "\n",
        "def get_city_id(city_df, city_name):\n",
        "    \"\"\"\n",
        "    Retrieve the city ID from the mapping CSV based on the city name.\n",
        "    \"\"\"\n",
        "    city_row = city_df[city_df['slug'] == slugify(city_name)]\n",
        "    if not city_row.empty:\n",
        "        return city_row.iloc[0, 0]  # Assuming city ID is in the first column\n",
        "    raise ValueError(f\"City '{city_name}' not found in the mapping file.\")\n",
        "\n",
        "\n",
        "\n",
        "def fetch_bus_data(token, from_id, to_id, date):\n",
        "    \"\"\"\n",
        "    Fetch bus data from the API for a given route and date.\n",
        "    Implements pagination to retrieve more than 100 records.\n",
        "    \"\"\"\n",
        "    api_url = \"https://internal-vroute-cmc.vexere.com/v2/route\"\n",
        "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "\n",
        "    all_data = []\n",
        "    page = 1\n",
        "    pagesize = 100000  # adjust if needed\n",
        "    while True:\n",
        "        query_params = {\n",
        "            \"filter[from]\": from_id,\n",
        "            \"filter[to]\": to_id,\n",
        "            \"filter[date]\": format_api_date(date),\n",
        "            \"filter[online_ticket]\": 0,\n",
        "            \"filter[is_promotion]\": 0,\n",
        "            \"filter[covid_utility]\": 0,\n",
        "            \"filter[speaking_english_utility]\": 0,\n",
        "            \"filter[enabled_gps]\": 0,\n",
        "            \"filter[has_cop]\": 0,\n",
        "            \"filter[online_reserved]\": 0,\n",
        "            \"filter[suggestion]\": \"DEFAULT\",\n",
        "            \"filter[fare][min]\": 0,\n",
        "            \"filter[fare][max]\": 2000000,\n",
        "            \"filter[available_seat][min]\": 1,\n",
        "            \"filter[available_seat][max]\": 50,\n",
        "            \"filter[rating][min]\": 0,\n",
        "            \"filter[rating][max]\": 5,\n",
        "            \"filter[limousine]\": 0,\n",
        "            \"filter[has_unfixed_point]\": 0,\n",
        "            \"page\": page,\n",
        "            \"pagesize\": pagesize\n",
        "        }\n",
        "\n",
        "        response = requests.get(api_url, headers=headers, params=query_params)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to retrieve data for {date} on page {page}. Status Code: {response.status_code}\")\n",
        "            print(response.text)\n",
        "            break\n",
        "\n",
        "        data = response.json().get('data', [])\n",
        "        if not data:\n",
        "            # No more records to fetch\n",
        "            break\n",
        "\n",
        "        all_data.extend(data)\n",
        "        page += 1\n",
        "\n",
        "    return all_data\n",
        "\n",
        "\n",
        "# def process_bus_data(data):\n",
        "#     \"\"\"\n",
        "#     Process the API response data and count occurrences of each bus company.\n",
        "#     \"\"\"\n",
        "#     company_list = [route['company']['name'] for route in data if 'company' in route]\n",
        "#     return Counter(company_list)\n",
        "\n",
        "\n",
        "\n",
        "def process_bus_data(data):\n",
        "    \"\"\"\n",
        "    Process the API response data and, for each company, collect the unique available seat counts.\n",
        "\n",
        "    It assumes that each record in `data` contains a 'company' key and that the available seat count is\n",
        "    found in:\n",
        "        company['available_seat_info']['seat_type']['1']['total_available_seat']\n",
        "    as well as optionally in the 'seat_group' section.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary mapping each company name to a set of unique available seat counts.\n",
        "    \"\"\"\n",
        "    company_seat_counts = defaultdict(set)\n",
        "\n",
        "    for record in data:\n",
        "        company = record['company']['name']\n",
        "        if 'available_seat_info' not in record:\n",
        "            continue\n",
        "        seat_info = record['available_seat_info']['seat_type'].keys()\n",
        "        for key in seat_info:\n",
        "            seat_info = record['available_seat_info']['seat_type'][key]['total_available_seat']\n",
        "            company_seat_counts[company].add(seat_info)\n",
        "\n",
        "    seat_counts = {company: len(seats) for company, seats in company_seat_counts.items()}\n",
        "    return seat_counts\n",
        "\n",
        "\n",
        "def group_and_sum(dataframe):\n",
        "    \"\"\"\n",
        "    Groups the DataFrame by 'Company Name' and sums the 'Count' for each group.\n",
        "\n",
        "    Parameters:\n",
        "        dataframe (pd.DataFrame): The input DataFrame with columns 'Company Name' and 'Count'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with each company and the corresponding summed count.\n",
        "    \"\"\"\n",
        "    return dataframe.groupby('Company Name', as_index=False)['Count'].sum()\n",
        "\n",
        "\n",
        "def get_bus_count(token, from_destination, to_destination, dates=[\"27-02-2025\"], mapping_csv=\"mapping.csv\"):\n",
        "    \"\"\"\n",
        "    Get the number of bus trips from a source to a destination for multiple dates.\n",
        "    Saves the result as a CSV in the \"result\" directory.\n",
        "\n",
        "    :param token: API authentication token\n",
        "    :param from_destination: Departure city\n",
        "    :param to_destination: Destination city\n",
        "    :param dates: List of dates (default: [\"27-02-2025\"])\n",
        "    :param mapping_csv: Path to the city mapping CSV file\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    # Load city mapping file\n",
        "    try:\n",
        "        city_df = pd.read_csv(mapping_csv)\n",
        "        from_id = get_city_id(city_df, from_destination)\n",
        "        to_id = get_city_id(city_df, to_destination)\n",
        "    except (FileNotFoundError, ValueError) as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Ensure result directory exists\n",
        "    result_dir = \"result\"\n",
        "    os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "    all_data = []\n",
        "    total_count = 0\n",
        "\n",
        "    for date in dates:\n",
        "        data = fetch_bus_data(token, from_id, to_id, date)\n",
        "        company_counts = process_bus_data(data)\n",
        "        # print(company_counts)\n",
        "        if data == []:\n",
        "            print(f\"No data available from {from_destination} to {to_destination} in {date}.\")\n",
        "            continue\n",
        "        if not company_counts:\n",
        "            print(f\"No data available from {from_destination} to {to_destination} in {date}.\")\n",
        "            continue\n",
        "\n",
        "        df_counts = pd.DataFrame(sorted(company_counts.items(), key=lambda x: x[1], reverse=True),\n",
        "                                 columns=[\"Company Name\", \"Count\"])\n",
        "        total_count += df_counts[\"Count\"].sum()\n",
        "        all_data.append(df_counts)\n",
        "\n",
        "    if all_data == []:\n",
        "        return None\n",
        "    # Merge all dates into one DataFrame\n",
        "    if all_data:\n",
        "        final_df = pd.concat(all_data, ignore_index=True)\n",
        "        total_row = pd.DataFrame([[\"Total\", total_count]], columns=[\"Company Name\", \"Count\"])\n",
        "        group_and_sum_df = group_and_sum(final_df)\n",
        "        group_and_sum_df['routes'] = f\"{from_destination} - {to_destination}\"\n",
        "        group_and_sum_df['date'] = f\"{date}\"\n",
        "\n",
        "        df_final = pd.concat([final_df, total_row], ignore_index=True)\n",
        "        # Save results to CSV\n",
        "        final_file = f\"{result_dir}/{from_destination}_{to_destination}_all_dates.csv\"\n",
        "        df_final.to_csv(final_file, index=False)\n",
        "        # print(f\"All data saved to {final_file}\")\n",
        "    else:\n",
        "        print(\"No data to save.\")\n",
        "\n",
        "    return from_destination, to_destination, total_count, group_and_sum_df\n",
        "\n",
        "\n",
        "def get_best(df, top = 20):\n",
        "    # Create a normalized route column by sorting the two destinations\n",
        "    df['route'] = df.apply(\n",
        "        lambda row: tuple(sorted([row['from destination'], row['to destination']])), axis=1\n",
        "    )\n",
        "    # For each route group, get the index of the row with the highest total count\n",
        "    idx = df.groupby('route')['total count'].idxmax()\n",
        "\n",
        "    # Get the unique routes with their maximum count\n",
        "    df_unique = df.loc[idx]\n",
        "\n",
        "    # Sort the results by 'total count' in descending order and take the top 20\n",
        "    top_20 = df_unique.sort_values('total count', ascending=False).head(top)\n",
        "\n",
        "    return top_20\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDNneKULM-O-",
        "outputId": "8ba3675a-739b-4b09-8fcf-f9e7ee8c430a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No data available from An Giang to Bà Rịa-Vũng Tàu in 08-03-2025.\n",
            "No data available from An Giang to Bắc Giang in 08-03-2025.\n",
            "No data available from An Giang to Bắc Kạn in 08-03-2025.\n",
            "No data available from An Giang to Bạc Liêu in 08-03-2025.\n",
            "No data available from Bà Rịa-Vũng Tàu to An Giang in 08-03-2025.\n",
            "No data available from Bà Rịa-Vũng Tàu to Bắc Giang in 08-03-2025.\n",
            "No data available from Bà Rịa-Vũng Tàu to Bắc Kạn in 08-03-2025.\n",
            "No data available from Bắc Giang to An Giang in 08-03-2025.\n",
            "No data available from Bắc Giang to Bà Rịa-Vũng Tàu in 08-03-2025.\n",
            "No data available from Bắc Giang to Bắc Kạn in 08-03-2025.\n",
            "No data available from Bắc Giang to Bạc Liêu in 08-03-2025.\n",
            "No data available from Bắc Kạn to An Giang in 08-03-2025.\n",
            "No data available from Bắc Kạn to Bà Rịa-Vũng Tàu in 08-03-2025.\n",
            "No data available from Bắc Kạn to Bắc Giang in 08-03-2025.\n",
            "No data available from Bắc Kạn to Bạc Liêu in 08-03-2025.\n",
            "No data available from Bạc Liêu to An Giang in 08-03-2025.\n",
            "No data available from Bạc Liêu to Bắc Giang in 08-03-2025.\n",
            "No data available from Bạc Liêu to Bắc Kạn in 08-03-2025.\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "\n",
        "city_df = pd.read_csv(\"mapping.csv\")\n",
        "# city_list = [\"Đà Nẵng\", \"Quảng Nam\", \"Quảng Ngãi\", \"Bình Định\", \"Phú Yên\", \"Khánh Hòa\", \"Ninh Thuận\", \"Bình Thuận\", \"Kon Tum\", \"Gia Lai\", \"Đắk Lắk\", \"Đắk Nông\", \"Lâm Đồng\", \"Bình Phước\", \"Bình Dương\", \"Đồng Nai\", \"Tây Ninh\", \"Bà Rịa - Vũng Tàu\", \"Hồ Chí Minh\", \"Long An\", \"Tiền Giang\", \"Bến Tre\", \"Trà Vinh\", \"Vĩnh Long\", \"Đồng Tháp\", \"An Giang\", \"Kiên Giang\", \"Cần Thơ\", \"Hậu Giang\", \"Sóc Trăng\", \"Bạc Liêu\", \"Cà Mau\"]\n",
        "city_list = city_df['name'].tolist()\n",
        "pairs = itertools.product(city_list, repeat=2)\n",
        "date = [\"15-03-2025\"]\n",
        "token= \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ0eXAiOjIsInVzciI6ImZlIiwiY2lkIjoiYTRlYWM1MDAtMzYyNC0xMWU1LWFjOWUtMDkxMjRjNjAxMDEzIiwiZXhwIjoxNzQxMTkyOTI4fQ.yDp4xOGE7bnCeb5Ak-KtF-blheeFHx7ulXhaRiW8tjQ\"\n",
        "# Print pairs, skipping (i, i) pairs\n",
        "results = []\n",
        "df = pd.DataFrame(columns=['from destination', 'to destination', 'total count'])\n",
        "for pair in pairs:\n",
        "    if pair[0] != pair[1]:\n",
        "      try:\n",
        "        data_ = get_bus_count(token,pair[0], pair[1], date)\n",
        "        if data_ is not None:\n",
        "            a, b, c, d = data_\n",
        "        else:\n",
        "            continue\n",
        "        df.loc[len(df)] = [a, b, c]  # Inserts at the next available row index\n",
        "        results.append(d)\n",
        "      except Exception as e:\n",
        "        continue\n",
        "results = pd.concat(results, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYEy36uKM-O_",
        "outputId": "09940d34-db32-4074-aa77-af1318c5d288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Company Name  Count                      routes        date\n",
            "0    Tuấn Hiệp      3  Bà Rịa-Vũng Tàu - Bạc Liêu  08-03-2025\n",
            "1    Tuấn Hiệp      3  Bạc Liêu - Bà Rịa-Vũng Tàu  08-03-2025\n"
          ]
        }
      ],
      "source": [
        "# This is all the company name, number of count trip, for each date, for each route\n",
        "print(results)\n",
        "# Save the results to a CSV file\n",
        "# results.to_csv(\"results.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-c_nhlUM-O_",
        "outputId": "6b3eb94c-3b81-47ba-dd52-99cff85b61c0"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'openpyxl'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      5\u001b[39m sort_by_count = group_by_company.sort_values(\u001b[33m'\u001b[39m\u001b[33mCount\u001b[39m\u001b[33m'\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m).head(\u001b[32m20\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Save the result to a CSV file\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# save_file = f\"result/group_by_company.csv\"\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# group_by_company.to_csv(save_file, index=False)\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Save the result to a XLXS file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43msort_by_count\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresult/group_by_company.xlsx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/carrier_data_scrapping/.venv/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/carrier_data_scrapping/.venv/lib/python3.12/site-packages/pandas/core/generic.py:2417\u001b[39m, in \u001b[36mNDFrame.to_excel\u001b[39m\u001b[34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   2404\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mformats\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexcel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[32m   2406\u001b[39m formatter = ExcelFormatter(\n\u001b[32m   2407\u001b[39m     df,\n\u001b[32m   2408\u001b[39m     na_rep=na_rep,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2415\u001b[39m     inf_rep=inf_rep,\n\u001b[32m   2416\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2417\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2419\u001b[39m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2420\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2422\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2423\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2424\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2425\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2426\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/carrier_data_scrapping/.venv/lib/python3.12/site-packages/pandas/io/formats/excel.py:943\u001b[39m, in \u001b[36mExcelFormatter.write\u001b[39m\u001b[34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m    941\u001b[39m     need_save = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     writer = \u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    949\u001b[39m     need_save = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/code/carrier_data_scrapping/.venv/lib/python3.12/site-packages/pandas/io/excel/_openpyxl.py:57\u001b[39m, in \u001b[36mOpenpyxlWriter.__init__\u001b[39m\u001b[34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     46\u001b[39m     path: FilePath | WriteExcelBuffer | ExcelWriter,\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     56\u001b[39m     \u001b[38;5;66;03m# Use the openpyxl module as the Excel writer.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenpyxl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mworkbook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[32m     59\u001b[39m     engine_kwargs = combine_kwargs(engine_kwargs, kwargs)\n\u001b[32m     61\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m     62\u001b[39m         path,\n\u001b[32m     63\u001b[39m         mode=mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m         engine_kwargs=engine_kwargs,\n\u001b[32m     67\u001b[39m     )\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'openpyxl'"
          ]
        }
      ],
      "source": [
        "# This is group by company name, sum the number of count trip, for all date, for all route\n",
        "# If you want to group by company name, delete the 'routes' in the list group_by_condition\n",
        "group_by_condition = ['Company Name']\n",
        "group_by_company = results.groupby(group_by_condition, as_index=False)['Count'].sum()\n",
        "sort_by_count = group_by_company.sort_values('Count', ascending=False).head(20)\n",
        "# Save the result to a CSV file\n",
        "# save_file = f\"result/group_by_company.csv\"\n",
        "# group_by_company.to_csv(save_file, index=False)\n",
        "# Save the result to a XLXS file\n",
        "sort_by_count.to_excel(\"result/group_by_company.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7g62nIxKM-PA",
        "outputId": "56589322-e235-4127-a937-aab6be7f20e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  from destination to destination  total count                    route\n",
            "0          Đà Nẵng      Quảng Nam           27     (Quảng Nam, Đà Nẵng)\n",
            "1          Đà Nẵng     Quảng Ngãi           26    (Quảng Ngãi, Đà Nẵng)\n",
            "3        Quảng Nam     Quảng Ngãi           17  (Quảng Nam, Quảng Ngãi)\n"
          ]
        }
      ],
      "source": [
        "# This return top best 20, 50 number of route with the highest number of count trip\n",
        "# If you want to change the number of top, change the 'top' in the function get_best\n",
        "top_best = get_best(df, top = 20)\n",
        "print(top_best)\n",
        "# Save the result to a CSV file\n",
        "# save_file = f\"result/top_best.csv\"\n",
        "# top_best.to_csv(save_file, index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}